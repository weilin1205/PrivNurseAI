{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bc491d-4c2a-492a-a2f2-b2973db994af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch._dynamo.config.disable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3735da3a-4f07-472c-9703-10d43c734cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gc\n",
    "import psutil\n",
    "import time\n",
    "\n",
    "# --- Configuration ---\n",
    "INPUT_CSV_PATH = \"../training_dataset_cleaned.csv\"\n",
    "OUTPUT_CSV_PATH = \"Datasets-note-summary.csv\"\n",
    "BASE_MODEL = \"google/medgemma-27b-text-it\"\n",
    "BATCH_SAVE_SIZE = 50  # 每處理多少筆資料就保存一次\n",
    "MAX_ROWS_TO_PROCESS = 5000  # Total: 14541\n",
    "MEMORY_CLEANUP_FREQUENCY = 10  # 每處理多少筆就進行記憶體清理\n",
    "OOM_RETRY_ATTEMPTS = 3  # OOM時的重試次數\n",
    "OOM_COOLDOWN_SECONDS = 5  # OOM後的冷卻時間\n",
    "\n",
    "# --- Memory Management Utilities ---\n",
    "def get_gpu_memory_usage():\n",
    "    \"\"\"獲取GPU記憶體使用情況\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.cuda.memory_allocated() / 1024**3, torch.cuda.memory_reserved() / 1024**3\n",
    "    return 0, 0\n",
    "\n",
    "def aggressive_memory_cleanup():\n",
    "    \"\"\"積極的記憶體清理\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    gc.collect()\n",
    "\n",
    "def print_memory_stats(stage: str):\n",
    "    \"\"\"打印記憶體使用統計\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated, reserved = get_gpu_memory_usage()\n",
    "        # print(f\"[{stage}] GPU Memory - Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB\")\n",
    "    \n",
    "    # CPU記憶體\n",
    "    process = psutil.Process()\n",
    "    # cpu_memory = process.memory_info().rss / 1024**3\n",
    "    # print(f\"[{stage}] CPU Memory: {cpu_memory:.2f}GB\")\n",
    "\n",
    "# --- 1. Model and Tokenizer Loading ---\n",
    "print(f\"Loading base model: {BASE_MODEL}...\")\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL, \n",
    "        torch_dtype=torch.bfloat16, \n",
    "        attn_implementation='eager', \n",
    "        device_map=\"auto\",\n",
    "        # low_cpu_mem_usage=True  # 減少CPU記憶體使用\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "    \n",
    "    # 設置padding token\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    print(\"Model and tokenizer loaded successfully.\")\n",
    "    print_memory_stats(\"After Model Loading\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    print(\"Please ensure you have enough VRAM and the necessary libraries (accelerate, bitsandbytes) are installed.\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Distillation Prompt Template ---\n",
    "def get_distillation_prompt(raw_xml: str, human_summary: str) -> str:\n",
    "    \"\"\"\n",
    "    Creates the final prompt for the MedGemma model, focusing on editing for factuality\n",
    "    while preserving human-like style.\n",
    "    \"\"\"\n",
    "    return f\"\"\"<start_of_turn>user\n",
    "You are a senior Attending Physician in the Emergency Department. Your primary responsibility is to review and finalize a \"Treatment Course\" summary drafted by a junior doctor.\n",
    "\n",
    "**Your Golden Rule: Start with the `<DOCTOR_DRAFT>` and EDIT it. Do not discard it and write from scratch.** Your task is to apply your expertise to perfect this draft.\n",
    "\n",
    "**Your Editing Process (in order):**\n",
    "\n",
    "1.  **CORRECT THE FACTS (The \"Red Pen\" Rule):**\n",
    "    Go through the draft line by line. If a statement (a diagnosis, a finding, a medication) in the draft **cannot be verified** by the `<RAW_DATA_XML>`, you must either:\n",
    "    - **Correct it** with the factual information from the XML.\n",
    "    - **Delete it** entirely if it's a complete hallucination.\n",
    "\n",
    "2.  **AUGMENT FOR COMPLETENESS (The \"Checklist\" Rule):**\n",
    "    After correcting the facts, review your revised draft. Is any of the following critical information from the XML missing? If so, integrate it smoothly into the text.\n",
    "    - **Diagnosis:** The primary working diagnosis.\n",
    "    - **Key Examinations:** Clinically significant lab/imaging results.\n",
    "    - **Key Medications/Interventions:** Important treatments administered.\n",
    "    - **Consultations:** (See special rule below).\n",
    "    - **Disposition & Follow-up Plan:** The final outcome. (Always be written at the end)\n",
    "\n",
    "---\n",
    "**Special Rule for Consultations:**\n",
    "- **IF** a consultation occurred (e.g., OBYN, NEUR, CRS), the summary **MUST** mention it detailly and its outcome.\n",
    "- **IF NO** consultation is mentioned in the XML, the summary **MUST NOT** mention one.\n",
    "\n",
    "**Strict Exclusion Rule:**\n",
    "- **DO NOT** include routine vital signs.\n",
    "- Past medical history does not need to be included in the summary.\n",
    "- There is no need to include excessive laboratory data.\n",
    "\n",
    "**Final Formatting Rule:**\n",
    "- The final, edited summary **MUST** be a **single, concise paragraph**.\n",
    "\n",
    "---\n",
    "**Input Data:**\n",
    "\n",
    "<RAW_DATA_XML>\n",
    "{raw_xml}\n",
    "</RAW_DATA_XML>\n",
    "\n",
    "<DOCTOR_DRAFT>\n",
    "{human_summary}\n",
    "</DOCTOR_DRAFT>\n",
    "\n",
    "**Your Task:**\n",
    "Provide the final, edited \"Treatment Course\" summary. Output ONLY the polished, single-paragraph text.\n",
    "DO NOT Thought.\n",
    "\n",
    "<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "\n",
    "# --- 3. Core Distillation Function with Memory Management ---\n",
    "def distill_summary(input_text: str, output_text: str, model, tokenizer) -> str:\n",
    "    \"\"\"\n",
    "    Generates a distilled summary using the provided model with improved memory management.\n",
    "    \"\"\"\n",
    "    if not isinstance(input_text, str) or not isinstance(output_text, str):\n",
    "        return \"Error: Invalid input type.\"\n",
    "    \n",
    "    for attempt in range(OOM_RETRY_ATTEMPTS):\n",
    "        try:\n",
    "            # 在每次生成前清理記憶體\n",
    "            if attempt > 0:\n",
    "                print(f\"  Retry attempt {attempt + 1}/{OOM_RETRY_ATTEMPTS}\")\n",
    "                aggressive_memory_cleanup()\n",
    "                time.sleep(OOM_COOLDOWN_SECONDS)\n",
    "            \n",
    "            prompt = get_distillation_prompt(input_text, output_text)\n",
    "            \n",
    "            # 使用no_grad來節省記憶體\n",
    "            with torch.no_grad():\n",
    "                inputs = tokenizer(\n",
    "                    prompt, \n",
    "                    return_tensors=\"pt\",\n",
    "                    truncation=True,\n",
    "                    # max_length=4096,  # 限制輸入長度\n",
    "                    padding=False\n",
    "                ).to(model.device)\n",
    "                \n",
    "                input_ids = inputs[\"input_ids\"]\n",
    "                \n",
    "                # 調整生成參數以節省記憶體\n",
    "                outputs = model.generate(\n",
    "                    input_ids, \n",
    "                    max_new_tokens=1024,  # 減少最大生成長度\n",
    "                    temperature=0.5,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                    # use_cache=True,  # 使用KV cache\n",
    "                    # early_stopping=True  # 早期停止\n",
    "                )\n",
    "\n",
    "                input_length = input_ids.shape[1]\n",
    "                newly_generated_tokens = outputs[0, input_length:]\n",
    "                distilled_text = tokenizer.decode(newly_generated_tokens, skip_special_tokens=True)\n",
    "                \n",
    "                # 立即清理這次推理的記憶體\n",
    "                del inputs, input_ids, outputs, newly_generated_tokens\n",
    "                torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "                \n",
    "                return distilled_text.strip()\n",
    "                \n",
    "        except torch.cuda.OutOfMemoryError as e:\n",
    "            print(f\"  CUDA OOM Error on attempt {attempt + 1}: {e}\")\n",
    "            aggressive_memory_cleanup()\n",
    "            \n",
    "            if attempt == OOM_RETRY_ATTEMPTS - 1:\n",
    "                return f\"ERROR: CUDA OOM after {OOM_RETRY_ATTEMPTS} attempts - {str(e)[:100]}...\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Unexpected error on attempt {attempt + 1}: {e}\")\n",
    "            if attempt == OOM_RETRY_ATTEMPTS - 1:\n",
    "                return f\"ERROR: {str(e)[:200]}...\"\n",
    "\n",
    "# --- 4. Batch Save Function ---\n",
    "def save_batch_to_csv(distilled_data: list, output_path: str, is_first_batch: bool = False):\n",
    "    \"\"\"\n",
    "    保存批次資料到CSV檔案\n",
    "    \"\"\"\n",
    "    if not distilled_data:\n",
    "        return\n",
    "    \n",
    "    batch_df = pd.DataFrame(distilled_data)\n",
    "    \n",
    "    # 如果是第一批次或檔案不存在，寫入標題列\n",
    "    write_header = is_first_batch or not os.path.exists(output_path)\n",
    "    \n",
    "    # 使用 'a' 模式附加到現有檔案，或 'w' 模式建立新檔案\n",
    "    mode = 'w' if is_first_batch else 'a'\n",
    "    \n",
    "    batch_df.to_csv(\n",
    "        output_path, \n",
    "        mode=mode,\n",
    "        header=write_header,\n",
    "        index=False, \n",
    "        encoding='utf-8'\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ 已保存 {len(distilled_data)} 筆資料到 {output_path}\")\n",
    "    \n",
    "    # 清理DataFrame記憶體\n",
    "    del batch_df\n",
    "    gc.collect()\n",
    "\n",
    "# --- 5. Resume Function ---\n",
    "def get_resume_index(output_path: str) -> int:\n",
    "    \"\"\"\n",
    "    檢查輸出檔案，確定從哪個索引開始繼續處理\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_path):\n",
    "        return 0\n",
    "    \n",
    "    try:\n",
    "        existing_df = pd.read_csv(output_path)\n",
    "        resume_index = len(existing_df)\n",
    "        print(f\"發現現有進度檔案，將從第 {resume_index + 1} 筆開始繼續處理\")\n",
    "        del existing_df  # 清理記憶體\n",
    "        gc.collect()\n",
    "        return resume_index\n",
    "    except Exception as e:\n",
    "        print(f\"讀取現有檔案時發生錯誤: {e}\")\n",
    "        print(\"將從頭開始處理\")\n",
    "        return 0\n",
    "\n",
    "# --- 6. Main Execution Block ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Reading data from {INPUT_CSV_PATH}...\")\n",
    "    try:\n",
    "        df = pd.read_csv(INPUT_CSV_PATH, nrows=MAX_ROWS_TO_PROCESS)\n",
    "        \n",
    "        # Ensure the required columns exist\n",
    "        if 'input_text' not in df.columns or 'output_text' not in df.columns:\n",
    "            print(\"Error: CSV must contain 'input_text' and 'output_text' columns.\")\n",
    "            exit()\n",
    "            \n",
    "        print(f\"Loaded {len(df)} rows for processing\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {INPUT_CSV_PATH} was not found.\")\n",
    "        exit()\n",
    "\n",
    "    # 檢查是否有現有進度可以繼續\n",
    "    start_index = get_resume_index(OUTPUT_CSV_PATH)\n",
    "    \n",
    "    # 如果需要從中間開始，跳過已處理的資料\n",
    "    if start_index > 0:\n",
    "        df = df.iloc[start_index:].reset_index(drop=True)\n",
    "        print(f\"跳過前 {start_index} 筆已處理的資料，剩餘 {len(df)} 筆待處理\")\n",
    "\n",
    "    # Create a list to hold the current batch results\n",
    "    current_batch = []\n",
    "    total_processed = start_index\n",
    "    error_count = 0\n",
    "\n",
    "    print(\"Starting distillation process...\")\n",
    "    print_memory_stats(\"Process Start\")\n",
    "    \n",
    "    # Using tqdm for a progress bar\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Distilling Summaries\"):\n",
    "        input_text = row['input_text']\n",
    "        output_text = row['output_text']\n",
    "        \n",
    "        # 定期進行記憶體清理\n",
    "        if total_processed % MEMORY_CLEANUP_FREQUENCY == 0 and total_processed > 0:\n",
    "            # print(f\"\\n[記憶體清理] 處理了 {total_processed} 筆資料\")\n",
    "            # print_memory_stats(\"Before Cleanup\")\n",
    "            aggressive_memory_cleanup()\n",
    "            # print_memory_stats(\"After Cleanup\")\n",
    "        \n",
    "        try:\n",
    "            # Generate the new, factually grounded summary\n",
    "            # print(f\"\\n處理第 {total_processed + 1} 筆資料...\")\n",
    "            distilled_output = distill_summary(input_text, output_text, model, tokenizer)\n",
    "            \n",
    "            # 檢查是否為錯誤輸出\n",
    "            if distilled_output.startswith(\"ERROR:\"):\n",
    "                error_count += 1\n",
    "                print(f\"⚠️  處理失敗: {distilled_output}\")\n",
    "            \n",
    "            # Store the results in current batch\n",
    "            current_batch.append({\n",
    "                'input_text': input_text,\n",
    "                'original_output': output_text,\n",
    "                'distilled_output': distilled_output\n",
    "            })\n",
    "            \n",
    "            total_processed += 1\n",
    "            \n",
    "            # 每處理 BATCH_SAVE_SIZE 筆就保存一次\n",
    "            if len(current_batch) >= BATCH_SAVE_SIZE:\n",
    "                print(f\"\\n=== 保存批次 (已處理 {total_processed} 筆) ===\")\n",
    "                is_first_batch = (total_processed == BATCH_SAVE_SIZE and start_index == 0)\n",
    "                save_batch_to_csv(current_batch, OUTPUT_CSV_PATH, is_first_batch)\n",
    "                current_batch = []  # 清空當前批次\n",
    "                aggressive_memory_cleanup()  # 批次保存後清理記憶體\n",
    "                print_memory_stats(\"After Batch Save\")\n",
    "                print(f\"錯誤數量: {error_count}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"處理第 {total_processed + 1} 筆資料時發生未預期錯誤: {e}\")\n",
    "            error_count += 1\n",
    "            \n",
    "            # Add a placeholder for the failed row\n",
    "            current_batch.append({\n",
    "                'input_text': input_text,\n",
    "                'original_output': output_text,\n",
    "                'distilled_output': f\"ERROR: Unexpected - {e}\"\n",
    "            })\n",
    "            total_processed += 1\n",
    "            \n",
    "            # 即使出錯也要保存批次\n",
    "            if len(current_batch) >= BATCH_SAVE_SIZE:\n",
    "                is_first_batch = (total_processed == BATCH_SAVE_SIZE and start_index == 0)\n",
    "                save_batch_to_csv(current_batch, OUTPUT_CSV_PATH, is_first_batch)\n",
    "                current_batch = []\n",
    "                aggressive_memory_cleanup()\n",
    "\n",
    "    # 保存剩餘的資料（最後一批可能不足 BATCH_SAVE_SIZE 筆）\n",
    "    if current_batch:\n",
    "        print(f\"\\n=== 保存最後批次 ({len(current_batch)} 筆) ===\")\n",
    "        is_first_batch = (start_index == 0 and total_processed <= BATCH_SAVE_SIZE)\n",
    "        save_batch_to_csv(current_batch, OUTPUT_CSV_PATH, is_first_batch)\n",
    "        aggressive_memory_cleanup()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Distillation process complete.\")\n",
    "    print(f\"Final dataset saved to {OUTPUT_CSV_PATH}\")\n",
    "    print(f\"Total processed records: {total_processed}\")\n",
    "    print(f\"Total errors: {error_count}\")\n",
    "    print(f\"Success rate: {((total_processed - error_count) / total_processed * 100):.2f}%\")\n",
    "    print_memory_stats(\"Process Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a17fb6-6cae-4d6c-99c2-9fe4c410583f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- 設定 ---\n",
    "# 你剛剛生成的蒸餾後檔案路徑\n",
    "DISTILLED_CSV_PATH = \"training_dataset_distilled_cleaned.csv\"\n",
    "# 你想查看的資料筆數\n",
    "NUM_RECORDS_TO_SHOW = 500\n",
    "\n",
    "# --- 主程式 ---\n",
    "print(f\"正在讀取檔案: {DISTILLED_CSV_PATH}\")\n",
    "\n",
    "try:\n",
    "    # 讀取 CSV 檔案\n",
    "    df = pd.read_csv(DISTILLED_CSV_PATH)\n",
    "\n",
    "    print(f\"檔案讀取成功，共 {len(df)} 筆資料。\")\n",
    "    print(f\"以下顯示前 {NUM_RECORDS_TO_SHOW} 筆資料的詳細對比：\\n\")\n",
    "\n",
    "    # 使用 .head() 來選取前 N 筆資料並進行迭代\n",
    "    for index, row in df.head(NUM_RECORDS_TO_SHOW).iterrows():\n",
    "        print(f\"==================  資料索引 {index}  ==================\")\n",
    "        # print(row['input_text'])\n",
    "        \n",
    "        # 印出原始的、可能帶有幻覺的摘要\n",
    "        # print(\"\\n【原始摘要 (Original Summary)】:\")\n",
    "        # print(row['original_output'])\n",
    "        \n",
    "        # 印出經過 MedGemma 蒸餾後的、基於事實的摘要\n",
    "        print(\"\\n【蒸餾後摘要 (Distilled Summary)】:\")\n",
    "        print(row['distilled_output'])\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"錯誤：找不到檔案 '{DISTILLED_CSV_PATH}'。請確認檔案名稱和路徑是否正確。\")\n",
    "except KeyError as e:\n",
    "    print(f\"錯誤：CSV 檔案中找不到欄位 {e}。請確認你的 CSV 檔案包含 'original_output' 和 'distilled_output' 欄位。\")\n",
    "except Exception as e:\n",
    "    print(f\"發生未知錯誤: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d956c8e-e230-4a13-90cb-6b1b20c96b6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Dataset Cleaner for Emergency Discharge Summaries\n",
    "Removes rows where distilled_output starts with \"ERROR\"\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from typing import Union, List, Dict\n",
    "\n",
    "def clean_json_dataset(input_file: str, output_file: str = None) -> int:\n",
    "    \"\"\"\n",
    "    Clean JSON dataset by removing rows with ERROR in distilled_output\n",
    "    \n",
    "    Args:\n",
    "        input_file: Path to input JSON file\n",
    "        output_file: Path to output JSON file (optional, defaults to input_file_cleaned.json)\n",
    "    \n",
    "    Returns:\n",
    "        Number of rows removed\n",
    "    \"\"\"\n",
    "    if output_file is None:\n",
    "        name, ext = os.path.splitext(input_file)\n",
    "        output_file = f\"{name}_cleaned{ext}\"\n",
    "    \n",
    "    # Load the dataset\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    original_count = len(data)\n",
    "    print(f\"Original dataset size: {original_count} rows\")\n",
    "    \n",
    "    # Filter out rows where distilled_output starts with \"ERROR\"\n",
    "    cleaned_data = []\n",
    "    removed_count = 0\n",
    "    \n",
    "    for item in data:\n",
    "        distilled_output = item.get('distilled_output', '')\n",
    "        \n",
    "        # Check if distilled_output starts with \"ERROR\" (case-insensitive)\n",
    "        if isinstance(distilled_output, str) and distilled_output.strip().upper().startswith('ERROR'):\n",
    "            removed_count += 1\n",
    "            print(f\"Removing row with ERROR: {distilled_output[:100]}...\")\n",
    "        else:\n",
    "            cleaned_data.append(item)\n",
    "    \n",
    "    # Save cleaned dataset\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(cleaned_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"Cleaned dataset size: {len(cleaned_data)} rows\")\n",
    "    print(f\"Removed {removed_count} rows with ERROR\")\n",
    "    print(f\"Cleaned dataset saved to: {output_file}\")\n",
    "    \n",
    "    return removed_count\n",
    "\n",
    "def clean_csv_dataset(input_file: str, output_file: str = None) -> int:\n",
    "    \"\"\"\n",
    "    Clean CSV dataset by removing rows with ERROR in distilled_output\n",
    "    \n",
    "    Args:\n",
    "        input_file: Path to input CSV file\n",
    "        output_file: Path to output CSV file (optional, defaults to input_file_cleaned.csv)\n",
    "    \n",
    "    Returns:\n",
    "        Number of rows removed\n",
    "    \"\"\"\n",
    "    if output_file is None:\n",
    "        name, ext = os.path.splitext(input_file)\n",
    "        output_file = f\"{name}_cleaned{ext}\"\n",
    "    \n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(input_file)\n",
    "    original_count = len(df)\n",
    "    print(f\"Original dataset size: {original_count} rows\")\n",
    "    \n",
    "    # Check if distilled_output column exists\n",
    "    if 'distilled_output' not in df.columns:\n",
    "        print(\"Warning: 'distilled_output' column not found!\")\n",
    "        print(f\"Available columns: {list(df.columns)}\")\n",
    "        return 0\n",
    "    \n",
    "    # Find rows where distilled_output starts with \"ERROR\"\n",
    "    error_mask = df['distilled_output'].astype(str).str.strip().str.upper().str.startswith('ERROR')\n",
    "    removed_count = error_mask.sum()\n",
    "    \n",
    "    # Show some examples of what's being removed\n",
    "    if removed_count > 0:\n",
    "        print(f\"\\nExamples of rows being removed:\")\n",
    "        error_rows = df[error_mask]['distilled_output'].head(3)\n",
    "        for i, row in enumerate(error_rows):\n",
    "            print(f\"  {i+1}. {str(row)[:100]}...\")\n",
    "    \n",
    "    # Remove rows with ERROR\n",
    "    cleaned_df = df[~error_mask].copy()\n",
    "    \n",
    "    # Save cleaned dataset\n",
    "    cleaned_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"\\nCleaned dataset size: {len(cleaned_df)} rows\")\n",
    "    print(f\"Removed {removed_count} rows with ERROR\")\n",
    "    print(f\"Cleaned dataset saved to: {output_file}\")\n",
    "    \n",
    "    return removed_count\n",
    "\n",
    "def clean_dataset_auto(input_file: str, output_file: str = None) -> int:\n",
    "    \"\"\"\n",
    "    Automatically detect file format and clean dataset\n",
    "    \n",
    "    Args:\n",
    "        input_file: Path to input file (JSON or CSV)\n",
    "        output_file: Path to output file (optional)\n",
    "    \n",
    "    Returns:\n",
    "        Number of rows removed\n",
    "    \"\"\"\n",
    "    if not os.path.exists(input_file):\n",
    "        raise FileNotFoundError(f\"Input file not found: {input_file}\")\n",
    "    \n",
    "    # Detect file format\n",
    "    _, ext = os.path.splitext(input_file)\n",
    "    ext = ext.lower()\n",
    "    \n",
    "    if ext == '.json':\n",
    "        return clean_json_dataset(input_file, output_file)\n",
    "    elif ext == '.csv':\n",
    "        return clean_csv_dataset(input_file, output_file)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {ext}. Please use .json or .csv files.\")\n",
    "\n",
    "def preview_errors(input_file: str, max_preview: int = 100) -> None:\n",
    "    \"\"\"\n",
    "    Preview rows that would be removed without actually cleaning\n",
    "    \n",
    "    Args:\n",
    "        input_file: Path to input file\n",
    "        max_preview: Maximum number of error rows to preview\n",
    "    \"\"\"\n",
    "    _, ext = os.path.splitext(input_file)\n",
    "    ext = ext.lower()\n",
    "    \n",
    "    print(f\"Previewing ERROR rows in {input_file}:\")\n",
    "    \n",
    "    if ext == '.json':\n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        error_count = 0\n",
    "        for i, item in enumerate(data):\n",
    "            distilled_output = item.get('distilled_output', '')\n",
    "            if isinstance(distilled_output, str) and distilled_output.strip().upper().startswith('ERROR'):\n",
    "                error_count += 1\n",
    "                if error_count <= max_preview:\n",
    "                    print(f\"  Row {i}: {distilled_output[:100]}...\")\n",
    "        \n",
    "        print(f\"\\nTotal ERROR rows found: {error_count}\")\n",
    "        \n",
    "    elif ext == '.csv':\n",
    "        df = pd.read_csv(input_file)\n",
    "        if 'distilled_output' not in df.columns:\n",
    "            print(\"Warning: 'distilled_output' column not found!\")\n",
    "            return\n",
    "        \n",
    "        error_mask = df['distilled_output'].astype(str).str.strip().str.upper().str.startswith('ERROR')\n",
    "        error_rows = df[error_mask]\n",
    "        \n",
    "        print(f\"Total ERROR rows found: {len(error_rows)}\")\n",
    "        \n",
    "        for i, (idx, row) in enumerate(error_rows.head(max_preview).iterrows()):\n",
    "            print(f\"  Row {idx}: {str(row['distilled_output'])[:100]}...\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage - replace with your file path\n",
    "    input_file = \"training_dataset_distilled.csv\"  # or \"your_dataset.csv\"\n",
    "    \n",
    "    try:\n",
    "        # Preview what will be removed (optional)\n",
    "        print(\"=== PREVIEW MODE ===\")\n",
    "        preview_errors(input_file)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        \n",
    "        # Clean the dataset\n",
    "        print(\"=== CLEANING DATASET ===\")\n",
    "        removed_count = clean_dataset_auto(input_file)\n",
    "        \n",
    "        if removed_count > 0:\n",
    "            print(f\"\\n✅ Successfully cleaned dataset! Removed {removed_count} ERROR rows.\")\n",
    "        else:\n",
    "            print(\"\\n✅ Dataset is already clean - no ERROR rows found.\")\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(\"❌ Please update the 'input_file' variable with your actual file path.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {str(e)}\")\n",
    "\n",
    "# Quick function for direct use\n",
    "def quick_clean(file_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Quick one-line function to clean a dataset\n",
    "    \n",
    "    Usage:\n",
    "        quick_clean(\"my_dataset.json\")\n",
    "    \"\"\"\n",
    "    try:\n",
    "        removed = clean_dataset_auto(file_path)\n",
    "        print(f\"✅ Quick clean completed! Removed {removed} ERROR rows.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Quick clean failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01e2093-d78e-440f-9aba-d2e23bf75b6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
