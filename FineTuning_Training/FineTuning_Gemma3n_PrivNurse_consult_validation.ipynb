{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b86bac-66c7-4c33-a6ab-15373bec7ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import wandb\n",
    "login(token = \"YOUR_KEY\")\n",
    "wandb.login(key = \"YOUR_KEY\")\n",
    "run = wandb.init(\n",
    "    project='Fine-tune gemma-3n-e4b',\n",
    "    job_type=\"training\",\n",
    "    anonymous=\"allow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f64ba8-a301-4293-ba44-efe0fecc79af",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_name = \"consult_validation_cleaned.csv\"\n",
    "base_model_name = \"unsloth/gemma-3n-E4B-it\"\n",
    "new_model_name = \"gemma-3n-privnurse-consult-validation-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d560ced0-4453-418f-a5cd-b7d3f3e55295",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastModel\n",
    "import torch\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = base_model_name,\n",
    "    dtype = None, # None for auto detection\n",
    "    max_seq_length = 8192, # Choose any for long context!\n",
    "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ab268a-3f99-419e-9229-5b872138a3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Let's finetune Gemma 3N #\n",
    "###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc27fce-ef48-457c-b6e1-0ad976151901",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = False, # Turn off for just text!\n",
    "    finetune_language_layers   = True,  # Should leave on!\n",
    "    finetune_attention_modules = True,  # Attention good for GRPO\n",
    "    finetune_mlp_modules       = True,  # SHould leave on always!\n",
    "\n",
    "    r = 32,           # Larger = higher accuracy, but might overfit\n",
    "    lora_alpha = 64,  # Recommended alpha == r at least\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb7996c-e3e7-476d-be62-271a70600da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"gemma-3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571b5a34-8b7c-446b-b464-bd9010e15488",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('csv', data_files=csv_file_name, split=\"all\")\n",
    "dataset = dataset.shuffle(seed=56)\n",
    "dataset[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb07b52-5938-443d-80c1-b91dbb3937f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System message for the assistant \n",
    "system_prompt = \"Extract the exact phrases or sentences from the [#會診申請單] text that correspond to the information summarized in the [#護理師確認結果]. Present these extracted phrases as an array of strings in JSON format, with the key 'relevant_text'. Do not output anything other than the JSON array.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a34d87-291a-4a14-bea0-2ad83c283768",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    # 構建 conversations 列表，每個元素包含 system, user, assistant\n",
    "    convos = []\n",
    "    for i in range(len(examples[\"original\"])):\n",
    "        convo = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": f\"#會診申請單：\\n{examples['original'][i]}\\n\\n#護理師確認結果：\\n{examples['summary'][i]}\"},\n",
    "            {\"role\": \"assistant\", \"content\": examples[\"claude_output\"][i]}\n",
    "        ]\n",
    "        convos.append(convo)\n",
    "    \n",
    "    # 應用 chat template 並移除 <bos> 前綴\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False).removeprefix('<bos>') for convo in convos]\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "# 使用批次處理\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0edc42-1bec-4e46-89e0-8ab913b06962",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[100][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43182667-69cf-4eb1-ab7b-46680f45674e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# Train the model #\n",
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1d2900-3d83-4b84-b4aa-c1d72d1f3a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "training_arguments = SFTConfig(\n",
    "    dataset_text_field=\"text\",\n",
    "    output_dir=new_model_name,\n",
    "    per_device_train_batch_size=6,     # 預設2\n",
    "    gradient_accumulation_steps=8,     # 預設8\n",
    "    optim=\"adamw_torch_fused\",         # Options: adamw_hf, adamw_torch, adamw_torch_fused, adamw_8bit\n",
    "    num_train_epochs=6,                \n",
    "    ###############\n",
    "    # per_device_eval_batch_size=2,    # 預設2\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    eval_strategy='no',\n",
    "    # eval_steps=10,\n",
    "    ###############\n",
    "    lr_scheduler_type = \"linear\",      \n",
    "    max_grad_norm=0.3,                 # Default: 0.3\n",
    "    warmup_ratio=0.1,                 # Default: 0.03\n",
    "    # warmup_steps=30,                 # Default: 30\n",
    "    learning_rate=1e-3,                # Default: 2e-4\n",
    "    # weight_decay=0.01,               # [new]Default: 0\n",
    "    # adam_beta1=0.9,                  # [new]Default: 0.9\n",
    "    # adam_beta2=0.95,                 # [new]Default: 0.999\n",
    "    # seed=1205,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=4,                # When batch size is big, logging steps should be reduced to 2-5 not 10\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    # group_by_length=True,\n",
    "    report_to=\"wandb\",\n",
    "    max_seq_length=8192,\n",
    "    seed = 3407,\n",
    "    # packing= False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee22f934-6687-4bba-b110-fef8109be516",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    eval_dataset = None, # Can set up evaluation!\n",
    "    args = training_arguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eceacfb-a1b2-4bf8-babd-3f6929778eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<start_of_turn>user\\n\",\n",
    "    response_part = \"<start_of_turn>model\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb211bed-803a-4154-8d01-fba645ce40eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(trainer.train_dataset[100][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b113cf-e44d-4544-aea7-33b0f9f19c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[100][\"labels\"]]).replace(tokenizer.pad_token, \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563dbb15-26c2-4e88-94a0-2dab47a250c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Bug]: InductorError: RuntimeError: Failed to run autotuning code block: PY_SSIZE_T_CLEAN macro must be defined for '#' formats\n",
    "# Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n",
    "# [Solution]: Disable TorchInductor Compilation. Add this before your inference code:\n",
    "import torch\n",
    "torch._dynamo.config.disable = True\n",
    "# Or alternatively, disable specific optimizations\n",
    "# torch._dynamo.config.suppress_errors = True\n",
    "# >> The error is related to PyTorch's aggressive optimization trying to compile the vision components of Gemma 3, which can be disabled without affecting functionality.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca925e8-0d15-4f57-8832-2d3c3ed46ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a966567-cc3b-4883-b746-8900961728b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d9a62e-cfed-46f1-88ea-fc58c66155a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac0979b-7472-4970-9625-cd3bddcc69e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632c0d3f-b076-4acf-9ac7-436107ea1e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "# Inference #\n",
    "#############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8987e3a1-be86-49d2-9c5d-210bfe564f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Bug]: InductorError: RuntimeError: Failed to run autotuning code block: PY_SSIZE_T_CLEAN macro must be defined for '#' formats\n",
    "# Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n",
    "# [Solution]: Disable TorchInductor Compilation. Add this before your inference code:\n",
    "import torch\n",
    "torch._dynamo.config.disable = True\n",
    "# Or alternatively, disable specific optimizations\n",
    "# torch._dynamo.config.suppress_errors = True\n",
    "# >> The error is related to PyTorch's aggressive optimization trying to compile the vision components of Gemma 3, which can be disabled without affecting functionality.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9720a7f5-0a3a-4dce-b9fa-808edea6557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "from transformers import TextStreamer\n",
    "\n",
    "def process_consultation(model, tokenizer, consultation_content, max_tokens=2048):\n",
    "    tokenizer = get_chat_template(\n",
    "        tokenizer,\n",
    "        chat_template = \"gemma-3\",\n",
    "    )\n",
    "    messages = [\n",
    "        {\"role\": \"system\",  \"content\": [{\"type\": \"text\", \"text\": system_prompt}]},\n",
    "        {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": consultation_content}]}\n",
    "    ]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt = True, # Must add for generation\n",
    "        return_tensors = \"pt\",\n",
    "        tokenize = True,\n",
    "        return_dict = True,\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    _ = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens = max_tokens,\n",
    "        ##############################################################################\n",
    "        # temperature = 1.0, top_p = 0.95, top_k = 64, # Recommended Gemma-3 settings!\n",
    "        ##############################################################################\n",
    "        temperature = 0.1,\n",
    "        top_p = 0.95,\n",
    "        top_k = 64,\n",
    "        ##############################################################################\n",
    "        streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19930383-5eb2-4c80-995c-e465f4c2db06",
   "metadata": {},
   "outputs": [],
   "source": [
    "consultation_content = \"\"\"\n",
    "#會診申請單：\n",
    "申請會診內容：\n",
    "Dear Dr\n",
    "This 45 years old woman admitted due to influenza A infection, beaause of progressive desaturation with fever on and off, follow CXR increased bilateral infiltration, under tapimycin, we need your consult for continue Tapimycin therapy. thanks a lot.\n",
    "\n",
    "被會診諮詢的科別：感染科\n",
    "\n",
    "回覆醫師：楊XX\n",
    "\n",
    "感染科回覆會診內容：\n",
    "Agree with continuing Tapimycin 4.5 gm iv q8h. (8523)\n",
    "\n",
    "護理師確認結果：\n",
    "病患因流感A感染入院，因持續低氧及間歇性發燒，胸部X光顯示雙側浸潤增加，申請感染科會診，感染科楊清鎮醫師回覆，同意繼續使用Tapimycin 4.5 gm iv q8h。\n",
    "\"\"\"\n",
    "process_consultation(model, tokenizer, consultation_content)\n",
    "# {\"relevant_text\": [\"This 45 years old woman admitted due to influenza A infection, beaause of progressive desaturation \n",
    "# with fever on and off\", \"follow CXR increased bilateral infiltration\", \"we need your consult for continue Tapimycin therapy\", \n",
    "# \"Agree with continuing Tapimycin 4.5 gm iv q8h. (8523)\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f21a2b-e58a-4670-b49c-56a144449261",
   "metadata": {},
   "outputs": [],
   "source": [
    "consultation_content = \"\"\"\n",
    "#會診申請單：\n",
    "申請會診內容：\n",
    "Dear doctor:today anti maturity \n",
    "This is 67y/o femlae becasue fell accident after discharge on 7/14. Now, Severe low back pain(VAS 8) and cannot walk now. Wheel chair+ .patient severe lower back pain and right leg soreness .follow L+T spine MRI revealed suspect L3-4 infection .today lab data get worse .spsepct sepsis .so we need your evalaution for antib use  .\n",
    "Thanks.\n",
    "8/13: anti 到期 thanks \n",
    "\n",
    "被會診諮詢的科別：感染科\n",
    "\n",
    "回覆醫師：楊XX\n",
    "\n",
    "感染科回覆會診內容：\n",
    "Agree with continuing Mepem 500 mg iv Q12h + Tecopin 400 mg iv Q3d. (8523)\n",
    "\n",
    "護理師確認結果：\n",
    "病患因今日抗生素到期，申請感染科會診。感染科楊XX醫師回覆，同意繼續使用Mepem 500 mg iv Q12h及Tecopin 400 mg iv Q3d。\n",
    "\"\"\"\n",
    "process_consultation(model, tokenizer, consultation_content)\n",
    "# {\"relevant_text\": [\"8/13: anti 到期 thanks\", \"Agree with continuing Mepem 500 mg iv Q12h + Tecopin 400 mg iv Q3d. (8523)\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbd4057-c445-4769-b0aa-9b73085a2980",
   "metadata": {},
   "outputs": [],
   "source": [
    "consultation_content = \"\"\"\n",
    "#會診申請單：\n",
    "申請會診內容：\n",
    "Dear Dr \n",
    "   This 68 y/o man is a case of bilateral pneumonia,abdomen fullness and Ileus \n",
    "noted \n",
    "we need your expertise for this patient \n",
    "Thank you very much  \n",
    "113/08/02\n",
    "stool impaction noted,we need your expertise for this patient \n",
    "Thank you very much \n",
    "\n",
    "被會診諮詢的科別：肝膽腸胃科\n",
    "\n",
    "回覆醫師：許XX\n",
    "\n",
    "肝膽腸胃科回覆會診內容：\n",
    "Dear : Doctor in charge.\n",
    "\n",
    "\n",
    "Impression :\n",
    "1. Suspect sigmoid volvulus due to chronic severe constipation.\n",
    "2. Constipation.\n",
    "\n",
    "\n",
    "Suggest : \n",
    "1. Consider to do colonoscopy with endoscopic detorsion first.\n",
    "2. Consider to do surgery if not improved with endoscopic detrosion.\n",
    "\n",
    "\n",
    "Thank you for yours consultation !!!\n",
    "\n",
    "# 護理師確認結果：\n",
    "病患因雙側肺炎、腹脹及腸阻塞，申請肝膽腸胃科會診。肝膽腸胃科許XX醫師回覆診斷為可能為乙狀結腸扭轉伴隨慢性嚴重便秘，並建議考慮進行結腸鏡內視鏡復位，若內視鏡復位無改善則考慮手術。\n",
    "\"\"\"\n",
    "process_consultation(model, tokenizer, consultation_content)\n",
    "# {\"relevant_text\": [\"This 68 y/o man is a case of bilateral pneumonia,abdomen fullness and Ileus noted\", \n",
    "# \"stool impaction noted,we need your expertise for this patient\", \"Suspect sigmoid volvulus due to chronic severe constipation.\", \n",
    "# \"Consider to do colonoscopy with endoscopic detorsion first.\", \"Consider to do surgery if not improved with endoscopic detrosion.\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cacca36-37c0-41ba-9215-5a9acfae3a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "consultation_content = \"\"\"\n",
    "#會診申請單：\n",
    "## 申請會診內容：\n",
    "Dear Dr.:\n",
    "  This is a 77 years old male patient who was admitted due to fever. Home test COVID Ag: (+) at home. He has history of 1. Type 2 diabetes mellitus, 2. Chronic kidney disease under medication. Laboratory data showed pyuria and acute kidney injury. Impression of 1. Urinary tract infection, 2. COVID-19 infection.\n",
    "Hence ,we need your assessment for antibiotic treatment, thanks a lot!\n",
    "\n",
    "for second course of Cravit, thank you.\n",
    "\n",
    "## 被會診諮詢的科別：感染科\n",
    "\n",
    "## 回覆醫師：陳XX\n",
    "\n",
    "## 感染科回覆會診內容：\n",
    "無\n",
    "\n",
    "# 護理師確認結果：\n",
    "會診感染科徵求抗生素使用建議，等待回覆\n",
    "\"\"\"\n",
    "process_consultation(model, tokenizer, consultation_content)\n",
    "# {\"relevant_text\": [\"Hence,we need your assessment for antibiotic treatment, thanks a lot!\", \"for second course of Cravit, thank you.\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3bb274-2a9d-40be-8f17-97ef3fa97424",
   "metadata": {},
   "outputs": [],
   "source": [
    "consultation_content = \"\"\"\n",
    "#會診申請單：\n",
    "申請會診內容：\n",
    "Dear Dr.\n",
    "This 65 years old male patient is a case of Enlarged prostate with lower urinary tract symptoms S/P Laser transurethral resection of the Prostate on 08/01\n",
    "Due to high fever and leukocytosis were noted, so we need your expert to this patient for evaluation \n",
    "血液檢查 Blood\n",
    "日     期         WBC             RBC         Hb     HCT      MCV \n",
    "= = = = =  ========== =============== ========== ======= ======== \n",
    "113/08/02   23730 uL  487 x10000/uL  13.7 g/dL  39.4 %  80.9 fl \n",
    "\n",
    "血液檢查 Blood\n",
    "日     期       MCH       MCHC       Platelet  RDW-CV     MPV \n",
    "= = = = =  ======== ========== ============== ======= ======= \n",
    "113/08/02   28.1 pg  34.8 g/dL  269 x1000/uL  13.3 %  8.9 fL \n",
    "\n",
    "血液檢查 Blood\n",
    "日     期   Neutrophil-Segmented  Lymphocyte  Monocyte \n",
    "= = = = =  ===================== =========== ========= \n",
    "113/08/02                 87.9 %       7.6 %     4.5 % \n",
    "\n",
    "血液檢查 Blood\n",
    "日     期   Absolute neutrophil count(ANC) \n",
    "= = = = =  =============================== \n",
    "113/08/02                        20859 /uL \n",
    "\n",
    "血液檢查 Blood\n",
    "Thank a lot!! \n",
    "\n",
    "被會診諮詢的科別：感染科\n",
    "\n",
    "回覆醫師：楊XX\n",
    "\n",
    "感染科回覆會診內容：\n",
    "Switch Stazolin iv to Seforce 400 mg iv Q12h (CODE: 8523).\n",
    "\n",
    "護理師確認結果：\n",
    "病患因高燒及白血球增多，申請感染科會診。感染科楊XX醫師回覆，同意將Stazolin iv改為Seforce 400 mg iv Q12h。\n",
    "\"\"\"\n",
    "process_consultation(model, tokenizer, consultation_content)\n",
    "# {\"relevant_text\": [\"Due to high fever and leukocytosis were noted, so we need your expert to this patient for evaluation\", \n",
    "# \"被會診諮詢的科別：感染科\", \"回覆醫師：楊XX\", \"Switch Stazolin iv to Seforce 400 mg iv Q12h (CODE: 8523).\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43781c6c-1abc-41dd-bea0-f26c625a9c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "############# Save the model #############\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89275b61-8e43-4bac-91b1-a4dd80974e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [NOTE] This ONLY saves the LoRA adapters, and not the full model. To save to GGUF, scroll down!\n",
    "model.save_pretrained(new_model_name)  # Local saving\n",
    "tokenizer.save_pretrained(new_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afd4e06-7497-4011-88f7-cc2012ef92c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained_merged(new_model_name, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35547e96-67ef-4628-84ff-96a4ceacd610",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.save_pretrained_gguf(\n",
    "    new_model_name,\n",
    "    quantization_type = \"Q8_0\", # For now only Q8_0, BF16, F16 supported\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26937063-a167-4cc9-9001-130dae605179",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
